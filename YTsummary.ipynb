{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Summarizing Youtube video using Gpt-3.5\nOnly use max 4000 character you can change the LLM model for larger input","metadata":{}},{"cell_type":"code","source":"!pip install openai\nimport openai\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-22T01:02:59.357353Z","iopub.execute_input":"2023-09-22T01:02:59.358043Z","iopub.status.idle":"2023-09-22T01:03:17.714268Z","shell.execute_reply.started":"2023-09-22T01:02:59.358007Z","shell.execute_reply":"2023-09-22T01:03:17.713024Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting openai\n  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: requests>=2.20 in /opt/conda/lib/python3.10/site-packages (from openai) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from openai) (4.66.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from openai) (3.8.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (2023.7.22)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\nInstalling collected packages: openai\nSuccessfully installed openai-0.28.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pip install pytube moviepy\n!pip install git+https://github.com/openai/whisper.git","metadata":{"execution":{"iopub.status.busy":"2023-09-22T01:09:10.639803Z","iopub.execute_input":"2023-09-22T01:09:10.640515Z","iopub.status.idle":"2023-09-22T01:10:37.650567Z","shell.execute_reply.started":"2023-09-22T01:09:10.640482Z","shell.execute_reply":"2023-09-22T01:10:37.649250Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (23.1.2)\nCollecting install\n  Downloading install-1.3.5-py3-none-any.whl (3.2 kB)\nCollecting pytube\n  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting moviepy\n  Downloading moviepy-1.0.3.tar.gz (388 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.3/388.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting decorator<5.0,>=4.0.2 (from moviepy)\n  Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\nRequirement already satisfied: tqdm<5.0,>=4.11.2 in /opt/conda/lib/python3.10/site-packages (from moviepy) (4.66.1)\nRequirement already satisfied: requests<3.0,>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from moviepy) (2.31.0)\nCollecting proglog<=1.0.0 (from moviepy)\n  Downloading proglog-0.1.10-py3-none-any.whl (6.1 kB)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from moviepy) (1.23.5)\nRequirement already satisfied: imageio<3.0,>=2.5 in /opt/conda/lib/python3.10/site-packages (from moviepy) (2.31.1)\nCollecting imageio_ffmpeg>=0.2.0 (from moviepy)\n  Downloading imageio_ffmpeg-0.4.9-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.10/site-packages (from imageio<3.0,>=2.5->moviepy) (9.5.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from imageio_ffmpeg>=0.2.0->moviepy) (68.0.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (2023.7.22)\nBuilding wheels for collected packages: moviepy\n  Building wheel for moviepy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for moviepy: filename=moviepy-1.0.3-py3-none-any.whl size=110730 sha256=5465b0219cd2b1e75a357860fe89aca127f4897b8a2afd6daeea9acb8a55d6df\n  Stored in directory: /root/.cache/pip/wheels/96/32/2d/e10123bd88fbfc02fed53cc18c80a171d3c87479ed845fa7c1\nSuccessfully built moviepy\n\u001b[33mWARNING: Error parsing requirements for beautifulsoup4: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/beautifulsoup4-4.12.2.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: pytube, proglog, install, imageio_ffmpeg, decorator, moviepy\n  Attempting uninstall: decorator\n    Found existing installation: decorator 5.1.1\n    Uninstalling decorator-5.1.1:\n      Successfully uninstalled decorator-5.1.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.9.0 which is incompatible.\njupyterlab-lsp 4.2.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed decorator-4.4.2 imageio_ffmpeg-0.4.9 install-1.3.5 moviepy-1.0.3 proglog-0.1.10 pytube-15.0.0\nCollecting git+https://github.com/openai/whisper.git\n  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-12h709pv\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-12h709pv\n  Resolved https://github.com/openai/whisper.git to commit 0a60fcaa9b86748389a656aa013c416030287d47\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting triton==2.0.0 (from openai-whisper==20230918)\n  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from openai-whisper==20230918) (0.57.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from openai-whisper==20230918) (1.23.5)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from openai-whisper==20230918) (2.0.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from openai-whisper==20230918) (4.66.1)\nRequirement already satisfied: more-itertools in /opt/conda/lib/python3.10/site-packages (from openai-whisper==20230918) (9.1.0)\nCollecting tiktoken==0.3.3 (from openai-whisper==20230918)\n  Downloading tiktoken-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken==0.3.3->openai-whisper==20230918) (2023.6.3)\nRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken==0.3.3->openai-whisper==20230918) (2.31.0)\nCollecting cmake (from triton==2.0.0->openai-whisper==20230918)\n  Downloading cmake-3.27.5-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.1/26.1 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->openai-whisper==20230918) (3.12.2)\nCollecting lit (from triton==2.0.0->openai-whisper==20230918)\n  Downloading lit-16.0.6.tar.gz (153 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.7/153.7 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->openai-whisper==20230918) (0.40.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->openai-whisper==20230918) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->openai-whisper==20230918) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->openai-whisper==20230918) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->openai-whisper==20230918) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230918) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230918) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230918) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230918) (2023.7.22)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->openai-whisper==20230918) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->openai-whisper==20230918) (1.3.0)\nBuilding wheels for collected packages: openai-whisper, lit\n  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for openai-whisper: filename=openai_whisper-20230918-py3-none-any.whl size=798399 sha256=4acbd7f38917111ff62d2ef08767f1232319bf207c14dd44a65221b9bcd9b7b3\n  Stored in directory: /tmp/pip-ephem-wheel-cache-i5x1nnyd/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n  Building wheel for lit (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for lit: filename=lit-16.0.6-py3-none-any.whl size=93584 sha256=db791b35564352ed59d45252e7aaa79a075c800bb82c452c9a1152e7cc444a63\n  Stored in directory: /root/.cache/pip/wheels/14/f9/07/bb2308587bc2f57158f905a2325f6a89a2befa7437b2d7e137\nSuccessfully built openai-whisper lit\n\u001b[33mWARNING: Error parsing requirements for beautifulsoup4: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/beautifulsoup4-4.12.2.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: lit, cmake, tiktoken, triton, openai-whisper\nSuccessfully installed cmake-3.27.5 lit-16.0.6 openai-whisper-20230918 tiktoken-0.3.3 triton-2.0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from pytube import YouTube\nfrom moviepy.editor import AudioFileClip\ndef youtube2audio (url: str):\n    yt = YouTube(url)\n    video = yt.streams.filter(abr='160kbps').last()\n    return video.download()","metadata":{"execution":{"iopub.status.busy":"2023-09-22T01:10:53.501339Z","iopub.execute_input":"2023-09-22T01:10:53.502025Z","iopub.status.idle":"2023-09-22T01:10:54.208148Z","shell.execute_reply.started":"2023-09-22T01:10:53.501972Z","shell.execute_reply":"2023-09-22T01:10:54.206950Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Variable Inputs\n#### * Enter your OpenAi API key\n#### * Youtube Video Link\n#### * Language","metadata":{}},{"cell_type":"code","source":"openai.api_key = ''\nvideo_link = \"https://www.youtube.com/watch?v=h5id4erwD4s\"\nlanguage = \"french\"","metadata":{"execution":{"iopub.status.busy":"2023-09-22T01:13:10.941985Z","iopub.execute_input":"2023-09-22T01:13:10.942398Z","iopub.status.idle":"2023-09-22T01:13:10.947437Z","shell.execute_reply.started":"2023-09-22T01:13:10.942365Z","shell.execute_reply":"2023-09-22T01:13:10.946426Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Convert Youtube video to Audio (mp3)","metadata":{}},{"cell_type":"code","source":"file_path = youtube2audio(video_link)\noutput_mp3_file = \"output.mp3\"\naudio_clip = AudioFileClip(file_path)\naudio_clip.write_audiofile(output_mp3_file, codec='mp3')\naudio_clip.close()","metadata":{"execution":{"iopub.status.busy":"2023-09-22T01:13:11.262148Z","iopub.execute_input":"2023-09-22T01:13:11.263200Z","iopub.status.idle":"2023-09-22T01:13:22.713210Z","shell.execute_reply.started":"2023-09-22T01:13:11.263163Z","shell.execute_reply":"2023-09-22T01:13:22.711807Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"MoviePy - Writing audio in output.mp3\n","output_type":"stream"},{"name":"stderr","text":"                                                                     ","output_type":"stream"},{"name":"stdout","text":"MoviePy - Done.\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}]},{"cell_type":"markdown","source":"### Audio to text using OpenAi Whisper","metadata":{}},{"cell_type":"code","source":"!whisper '/kaggle/working/output.mp3' --language en","metadata":{"execution":{"iopub.status.busy":"2023-09-22T01:13:48.456604Z","iopub.execute_input":"2023-09-22T01:13:48.457072Z","iopub.status.idle":"2023-09-22T01:14:50.449803Z","shell.execute_reply.started":"2023-09-22T01:13:48.457037Z","shell.execute_reply":"2023-09-22T01:14:50.448530Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n100%|███████████████████████████████████████| 461M/461M [00:05<00:00, 85.1MiB/s]\n[00:00.000 --> 00:02.720]  When you ask the large language model a question,\n[00:02.720 --> 00:04.520]  it is important that it retrieves\n[00:04.520 --> 00:07.320]  the most relevant information to answer it.\n[00:07.320 --> 00:11.080]  This is otherwise known as retrieval augmented generation.\n[00:11.080 --> 00:12.480]  For example, when you ask\n[00:12.480 --> 00:14.640]  Chatchapiti what the name of your dog is,\n[00:14.640 --> 00:16.200]  it has no clue.\n[00:16.200 --> 00:18.400]  But if you feed it the data that is\n[00:18.400 --> 00:20.520]  in stored in your vector database,\n[00:20.520 --> 00:25.160]  and maybe you specify that Erica has a dog and her name is Bowen,\n[00:25.160 --> 00:27.200]  Chatchapiti is able to retrieve\n[00:27.200 --> 00:30.320]  that relevant context and properly answer your question.\n[00:30.320 --> 00:32.520]  In this video, I'll go over how you can\n[00:32.520 --> 00:37.280]  chunk your text data to ensure that the language model is one,\n[00:37.280 --> 00:40.280]  receiving the most relevant information,\n[00:40.280 --> 00:43.240]  maybe instead of feeding it the whole PDF document,\n[00:43.240 --> 00:44.760]  you feed it by elements,\n[00:44.760 --> 00:46.240]  which I have a visual for,\n[00:46.240 --> 00:49.520]  and then two, you don't want to go over the context window,\n[00:49.520 --> 00:52.520]  and this is dependent on which model that you are using.\n[00:52.520 --> 00:54.160]  Starting with the text splitter.\n[00:54.200 --> 00:57.240]  So what this does is it takes the PDF document,\n[00:57.240 --> 00:59.720]  which we have here on the left-hand side,\n[00:59.720 --> 01:03.400]  and it takes the chunks depending on the model\n[01:03.400 --> 01:05.440]  or the chunk size that you defined.\n[01:05.440 --> 01:12.120]  So for example, GPT 3.5 Turbo 16K has a 16,000 token window,\n[01:12.120 --> 01:16.840]  but other models have either a smaller or larger window size.\n[01:16.840 --> 01:19.640]  So just what this is doing is just splitting up the text\n[01:19.640 --> 01:21.040]  once it hits that limit,\n[01:21.040 --> 01:24.720]  and then boom, we have chunk one, which is the blue text,\n[01:24.720 --> 01:28.320]  and then it moves right on to the green window,\n[01:28.320 --> 01:30.000]  which is the next chunk.\n[01:30.000 --> 01:33.960]  The rolling window complements the text splitter extremely well,\n[01:33.960 --> 01:37.040]  and this is because once chunk one is finished,\n[01:37.040 --> 01:41.400]  chunk two will begin with a few tokens or characters\n[01:41.400 --> 01:43.680]  that is included in chunk one.\n[01:43.680 --> 01:46.800]  So in this example, you can see the blue text\n[01:46.840 --> 01:51.560]  from this document is the beginning or the start of chunk two,\n[01:51.560 --> 01:54.040]  and this is very important in documents\n[01:54.040 --> 01:57.200]  where the second sentence doesn't make sense\n[01:57.200 --> 02:00.840]  because it doesn't have the information from sentence one.\n[02:00.840 --> 02:04.000]  Lama index, Lang chain, and Haystack,\n[02:04.000 --> 02:07.400]  they all three have this implemented in their frameworks,\n[02:07.400 --> 02:08.680]  and at the end of this video,\n[02:08.680 --> 02:11.560]  I'll share it with you with the link to the documentation\n[02:11.560 --> 02:13.000]  so you can test this out for yourself.\n[02:13.000 --> 02:15.840]  Moving on to how you would chunk PDF documents.\n[02:15.880 --> 02:18.440]  So previously, Chukrin I partnered up\n[02:18.440 --> 02:20.800]  on creating a demo for unstructured.\n[02:20.800 --> 02:23.640]  So what we did was we ingested PDF documents,\n[02:23.640 --> 02:25.800]  which were two research papers,\n[02:25.800 --> 02:28.600]  and then we chunked it by the elements,\n[02:28.600 --> 02:30.160]  which unstructured has.\n[02:30.160 --> 02:32.440]  And if you haven't used unstructured before,\n[02:32.440 --> 02:36.320]  I highly recommend checking it out along with Lama index.\n[02:36.320 --> 02:40.160]  So what it is doing is it takes the PDF heading.\n[02:40.160 --> 02:42.240]  So in this example, I have the abstract,\n[02:42.240 --> 02:43.800]  introduction, and related work.\n[02:43.800 --> 02:46.680]  So each section will have its own chunk.\n[02:46.680 --> 02:49.720]  And this is very important when making queries\n[02:49.720 --> 02:51.840]  that are specific to title.\n[02:51.840 --> 02:55.280]  So this is that kind of semantic region of searching.\n[02:55.280 --> 02:57.520]  And maybe you'd have like the abstract property,\n[02:57.520 --> 02:59.520]  introduction, property, et cetera.\n[02:59.520 --> 03:01.080]  And this is one way to make sure\n[03:01.080 --> 03:04.000]  that you have extremely relevant information.\n[03:04.000 --> 03:06.400]  If you're interested in using unstructured\n[03:06.400 --> 03:09.960]  with Lama index, so if you start typing in unstructured,\n[03:09.960 --> 03:13.080]  you can find the file loader here.\n[03:13.080 --> 03:16.000]  And this is where you can ingest TXT files,\n[03:16.000 --> 03:18.280]  doc, PowerPoint, JPEG, all of it.\n[03:18.280 --> 03:21.480]  Then moving on to the Lama index node parser.\n[03:21.480 --> 03:24.000]  So this is where you would define that chunk size\n[03:24.000 --> 03:27.160]  that I talked about, and then also the chunk overlap.\n[03:27.160 --> 03:30.240]  So with this chunk overlap of 20 tokens,\n[03:30.240 --> 03:33.120]  it is taking the 20 tokens from the previous chunk\n[03:33.120 --> 03:35.080]  into the following chunk.\n[03:35.080 --> 03:37.160]  And this is a lane chain documentation\n[03:37.160 --> 03:39.640]  on where they have their text splitters.\n[03:39.640 --> 03:42.960]  So they have, I believe, code on how to do this.\n[03:42.960 --> 03:46.640]  And maybe if you want to split the text by the new lines\n[03:46.640 --> 03:50.800]  or also the chunk overlap and the chunk size as well.\n[03:50.800 --> 03:53.080]  And then ending with haystack.\n[03:53.080 --> 03:55.200]  They have the chunk size,\n[03:55.200 --> 03:58.240]  and then again the split overlap.\n[03:58.240 --> 03:59.760]  So you can check that out.\n[03:59.760 --> 04:03.360]  And then I want to end this video\n[04:03.360 --> 04:06.960]  with recommending you join the Arise workshop\n[04:06.960 --> 04:09.920]  that I will be hosting along with Aman from Arise\n[04:09.920 --> 04:11.600]  and Ronnie from Unstructured.\n[04:11.640 --> 04:14.400]  We will be exploring the chunking techniques\n[04:14.400 --> 04:18.120]  and re-ranking for enhancing your retrieval\n[04:18.120 --> 04:21.040]  and the results in your FAG application.\n[04:21.040 --> 04:22.840]  I hope you guys are able to join.\n[04:22.840 --> 04:24.280]  And I'll see you next time.\n[04:24.280 --> 04:25.120]  Bye.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Util Function OpenAI GPT-3.5 Turbo model (or another specified model) to generate a text completion or response based on a given prompt. ","metadata":{}},{"cell_type":"code","source":"\ndef get_completion(prompt, model=\"gpt-3.5-turbo\"): # Andrew mentioned that the prompt/ completion paradigm is preferable for this class\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=messages,\n        temperature=0, # this is the degree of randomness of the model's output\n    )\n    return response.choices[0].message[\"content\"]","metadata":{"execution":{"iopub.status.busy":"2023-09-22T01:17:02.933918Z","iopub.execute_input":"2023-09-22T01:17:02.934357Z","iopub.status.idle":"2023-09-22T01:17:02.940771Z","shell.execute_reply.started":"2023-09-22T01:17:02.934325Z","shell.execute_reply":"2023-09-22T01:17:02.939734Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Open Text file ","metadata":{}},{"cell_type":"code","source":"#Input data\n# filepath = \"/kaggle/input/pdfdata/0_Game_of_Thrones__season_8_.txt\"\n# filepath = \"/kaggle/input/pdfdata/cv.pdf\"\nfilepath='/kaggle/working/output.txt'\nfile_contents=\"\"\nwith open(filepath, 'r') as file:\n    file_contents = file.read().strip()\nprint(file_contents)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-22T01:15:47.321412Z","iopub.execute_input":"2023-09-22T01:15:47.321855Z","iopub.status.idle":"2023-09-22T01:15:47.329828Z","shell.execute_reply.started":"2023-09-22T01:15:47.321818Z","shell.execute_reply":"2023-09-22T01:15:47.328695Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"When you ask the large language model a question,\nit is important that it retrieves\nthe most relevant information to answer it.\nThis is otherwise known as retrieval augmented generation.\nFor example, when you ask\nChatchapiti what the name of your dog is,\nit has no clue.\nBut if you feed it the data that is\nin stored in your vector database,\nand maybe you specify that Erica has a dog and her name is Bowen,\nChatchapiti is able to retrieve\nthat relevant context and properly answer your question.\nIn this video, I'll go over how you can\nchunk your text data to ensure that the language model is one,\nreceiving the most relevant information,\nmaybe instead of feeding it the whole PDF document,\nyou feed it by elements,\nwhich I have a visual for,\nand then two, you don't want to go over the context window,\nand this is dependent on which model that you are using.\nStarting with the text splitter.\nSo what this does is it takes the PDF document,\nwhich we have here on the left-hand side,\nand it takes the chunks depending on the model\nor the chunk size that you defined.\nSo for example, GPT 3.5 Turbo 16K has a 16,000 token window,\nbut other models have either a smaller or larger window size.\nSo just what this is doing is just splitting up the text\nonce it hits that limit,\nand then boom, we have chunk one, which is the blue text,\nand then it moves right on to the green window,\nwhich is the next chunk.\nThe rolling window complements the text splitter extremely well,\nand this is because once chunk one is finished,\nchunk two will begin with a few tokens or characters\nthat is included in chunk one.\nSo in this example, you can see the blue text\nfrom this document is the beginning or the start of chunk two,\nand this is very important in documents\nwhere the second sentence doesn't make sense\nbecause it doesn't have the information from sentence one.\nLama index, Lang chain, and Haystack,\nthey all three have this implemented in their frameworks,\nand at the end of this video,\nI'll share it with you with the link to the documentation\nso you can test this out for yourself.\nMoving on to how you would chunk PDF documents.\nSo previously, Chukrin I partnered up\non creating a demo for unstructured.\nSo what we did was we ingested PDF documents,\nwhich were two research papers,\nand then we chunked it by the elements,\nwhich unstructured has.\nAnd if you haven't used unstructured before,\nI highly recommend checking it out along with Lama index.\nSo what it is doing is it takes the PDF heading.\nSo in this example, I have the abstract,\nintroduction, and related work.\nSo each section will have its own chunk.\nAnd this is very important when making queries\nthat are specific to title.\nSo this is that kind of semantic region of searching.\nAnd maybe you'd have like the abstract property,\nintroduction, property, et cetera.\nAnd this is one way to make sure\nthat you have extremely relevant information.\nIf you're interested in using unstructured\nwith Lama index, so if you start typing in unstructured,\nyou can find the file loader here.\nAnd this is where you can ingest TXT files,\ndoc, PowerPoint, JPEG, all of it.\nThen moving on to the Lama index node parser.\nSo this is where you would define that chunk size\nthat I talked about, and then also the chunk overlap.\nSo with this chunk overlap of 20 tokens,\nit is taking the 20 tokens from the previous chunk\ninto the following chunk.\nAnd this is a lane chain documentation\non where they have their text splitters.\nSo they have, I believe, code on how to do this.\nAnd maybe if you want to split the text by the new lines\nor also the chunk overlap and the chunk size as well.\nAnd then ending with haystack.\nThey have the chunk size,\nand then again the split overlap.\nSo you can check that out.\nAnd then I want to end this video\nwith recommending you join the Arise workshop\nthat I will be hosting along with Aman from Arise\nand Ronnie from Unstructured.\nWe will be exploring the chunking techniques\nand re-ranking for enhancing your retrieval\nand the results in your FAG application.\nI hope you guys are able to join.\nAnd I'll see you next time.\nBye.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Basic Promt Example","metadata":{}},{"cell_type":"code","source":"\nprod_review = \"\"\"\nI recently purchased this colorful backpack for my son's school needs. \\\nHe absolutely loves it and carries it with him every day. The backpack is not only stylish but also very practical. \\\nIt has multiple compartments, which help him organize his school supplies efficiently.\n\nOne of the standout features is the comfortable shoulder straps. My son often has a heavy load of books, but the padded\\\nstraps make it easy for him to carry the backpack without discomfort. The vibrant design and quality materials make it a hit with his friends too.\n\nWhile the backpack is generally great, there's one minor drawback. The water bottle holder on the side is a bit tight,\\ \nmaking it challenging to fit a larger bottle. It would be nice if it were a bit more accommodating in this regard.\n\nShipping and delivery were smooth. It arrived on time, and the packaging was in excellent condition. Overall, I'm satisfied with this purchase,\\\nand it has made my son's school life more enjoyable.\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-09-22T01:19:52.888476Z","iopub.execute_input":"2023-09-22T01:19:52.888901Z","iopub.status.idle":"2023-09-22T01:19:52.895425Z","shell.execute_reply.started":"2023-09-22T01:19:52.888868Z","shell.execute_reply":"2023-09-22T01:19:52.894335Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"\nprompt = f\"\"\"\nYour task is to generate a short summary of a product \\\nreview from an ecommerce site. \n\nSummarize the review below, delimited by triple \nbackticks, in at most 30 words. \n\nReview: ```{prod_review}```\n\"\"\"\n\nresponse = get_completion(prompt)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2023-09-22T01:19:53.289574Z","iopub.execute_input":"2023-09-22T01:19:53.290184Z","iopub.status.idle":"2023-09-22T01:19:55.178095Z","shell.execute_reply.started":"2023-09-22T01:19:53.290151Z","shell.execute_reply":"2023-09-22T01:19:55.176887Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"This colorful and practical backpack with multiple compartments and comfortable shoulder straps is a hit with kids. The only drawback is the tight water bottle holder. Smooth shipping and delivery. Overall, a satisfying purchase.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Prompt\n* first Summerize data \n* Traslate to language input by the user","metadata":{}},{"cell_type":"code","source":"prompt = f\"\"\"\nYour objective is to summarize the text in file_contents \\\nThen convert the summary into {language} language.\n\nSummary :\nPlease limit your response to 100 words.\n\nUse the following format:\nEnglish:\n'''\nDisplay summary \n'''\nTranslate:\n'''\nDisplay Translation\n'''\n\nfile_contents:\n```{file_contents[:min(4000 , len(file_contents))]}```\n\"\"\"\n# print(prompt)","metadata":{"execution":{"iopub.status.busy":"2023-09-22T01:17:28.300736Z","iopub.execute_input":"2023-09-22T01:17:28.301132Z","iopub.status.idle":"2023-09-22T01:17:28.306839Z","shell.execute_reply.started":"2023-09-22T01:17:28.301099Z","shell.execute_reply":"2023-09-22T01:17:28.305430Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Summary","metadata":{}},{"cell_type":"code","source":"response = get_completion(prompt)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2023-09-22T01:17:31.201856Z","iopub.execute_input":"2023-09-22T01:17:31.203004Z","iopub.status.idle":"2023-09-22T01:17:53.155578Z","shell.execute_reply.started":"2023-09-22T01:17:31.202952Z","shell.execute_reply":"2023-09-22T01:17:53.154428Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"English:\nWhen using a large language model to answer questions, it is important to retrieve the most relevant information. This process is known as retrieval augmented generation. By providing the model with specific data, such as information stored in a vector database, it can retrieve the relevant context and answer the question accurately. Chunking the text data can help ensure that the model receives the most relevant information without exceeding the context window. Different models have different window sizes. The text splitter and rolling window techniques can be used to chunk PDF documents effectively. Joining the Arise workshop can provide further insights into chunking techniques and enhancing retrieval.\n\nTranslate:\nLorsque vous utilisez un grand modèle de langage pour répondre aux questions, il est important de récupérer les informations les plus pertinentes. Ce processus est appelé génération augmentée par récupération. En fournissant au modèle des données spécifiques, telles que des informations stockées dans une base de données vectorielle, il peut récupérer le contexte pertinent et répondre avec précision à la question. Le fractionnement des données textuelles peut aider à s'assurer que le modèle reçoit les informations les plus pertinentes sans dépasser la fenêtre contextuelle. Différents modèles ont différentes tailles de fenêtre. Les techniques de fractionnement de texte et de fenêtre roulante peuvent être utilisées pour fractionner efficacement les documents PDF. Rejoindre l'atelier Arise peut fournir des informations supplémentaires sur les techniques de fractionnement et l'amélioration de la récupération.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}