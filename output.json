{"text": " When you ask the large language model a question, it is important that it retrieves the most relevant information to answer it. This is otherwise known as retrieval augmented generation. For example, when you ask Chatchapiti what the name of your dog is, it has no clue. But if you feed it the data that is in stored in your vector database, and maybe you specify that Erica has a dog and her name is Bowen, Chatchapiti is able to retrieve that relevant context and properly answer your question. In this video, I'll go over how you can chunk your text data to ensure that the language model is one, receiving the most relevant information, maybe instead of feeding it the whole PDF document, you feed it by elements, which I have a visual for, and then two, you don't want to go over the context window, and this is dependent on which model that you are using. Starting with the text splitter. So what this does is it takes the PDF document, which we have here on the left-hand side, and it takes the chunks depending on the model or the chunk size that you defined. So for example, GPT 3.5 Turbo 16K has a 16,000 token window, but other models have either a smaller or larger window size. So just what this is doing is just splitting up the text once it hits that limit, and then boom, we have chunk one, which is the blue text, and then it moves right on to the green window, which is the next chunk. The rolling window complements the text splitter extremely well, and this is because once chunk one is finished, chunk two will begin with a few tokens or characters that is included in chunk one. So in this example, you can see the blue text from this document is the beginning or the start of chunk two, and this is very important in documents where the second sentence doesn't make sense because it doesn't have the information from sentence one. Lama index, Lang chain, and Haystack, they all three have this implemented in their frameworks, and at the end of this video, I'll share it with you with the link to the documentation so you can test this out for yourself. Moving on to how you would chunk PDF documents. So previously, Chukrin I partnered up on creating a demo for unstructured. So what we did was we ingested PDF documents, which were two research papers, and then we chunked it by the elements, which unstructured has. And if you haven't used unstructured before, I highly recommend checking it out along with Lama index. So what it is doing is it takes the PDF heading. So in this example, I have the abstract, introduction, and related work. So each section will have its own chunk. And this is very important when making queries that are specific to title. So this is that kind of semantic region of searching. And maybe you'd have like the abstract property, introduction, property, et cetera. And this is one way to make sure that you have extremely relevant information. If you're interested in using unstructured with Lama index, so if you start typing in unstructured, you can find the file loader here. And this is where you can ingest TXT files, doc, PowerPoint, JPEG, all of it. Then moving on to the Lama index node parser. So this is where you would define that chunk size that I talked about, and then also the chunk overlap. So with this chunk overlap of 20 tokens, it is taking the 20 tokens from the previous chunk into the following chunk. And this is a lane chain documentation on where they have their text splitters. So they have, I believe, code on how to do this. And maybe if you want to split the text by the new lines or also the chunk overlap and the chunk size as well. And then ending with haystack. They have the chunk size, and then again the split overlap. So you can check that out. And then I want to end this video with recommending you join the Arise workshop that I will be hosting along with Aman from Arise and Ronnie from Unstructured. We will be exploring the chunking techniques and re-ranking for enhancing your retrieval and the results in your FAG application. I hope you guys are able to join. And I'll see you next time. Bye.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 2.72, "text": " When you ask the large language model a question,", "tokens": [50364, 1133, 291, 1029, 264, 2416, 2856, 2316, 257, 1168, 11, 50500], "temperature": 0.0, "avg_logprob": -0.17598097441626376, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.004197871778160334}, {"id": 1, "seek": 0, "start": 2.72, "end": 4.5200000000000005, "text": " it is important that it retrieves", "tokens": [50500, 309, 307, 1021, 300, 309, 19817, 977, 50590], "temperature": 0.0, "avg_logprob": -0.17598097441626376, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.004197871778160334}, {"id": 2, "seek": 0, "start": 4.5200000000000005, "end": 7.32, "text": " the most relevant information to answer it.", "tokens": [50590, 264, 881, 7340, 1589, 281, 1867, 309, 13, 50730], "temperature": 0.0, "avg_logprob": -0.17598097441626376, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.004197871778160334}, {"id": 3, "seek": 0, "start": 7.32, "end": 11.08, "text": " This is otherwise known as retrieval augmented generation.", "tokens": [50730, 639, 307, 5911, 2570, 382, 19817, 3337, 36155, 5125, 13, 50918], "temperature": 0.0, "avg_logprob": -0.17598097441626376, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.004197871778160334}, {"id": 4, "seek": 0, "start": 11.08, "end": 12.48, "text": " For example, when you ask", "tokens": [50918, 1171, 1365, 11, 562, 291, 1029, 50988], "temperature": 0.0, "avg_logprob": -0.17598097441626376, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.004197871778160334}, {"id": 5, "seek": 0, "start": 12.48, "end": 14.64, "text": " Chatchapiti what the name of your dog is,", "tokens": [50988, 761, 852, 569, 8707, 437, 264, 1315, 295, 428, 3000, 307, 11, 51096], "temperature": 0.0, "avg_logprob": -0.17598097441626376, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.004197871778160334}, {"id": 6, "seek": 0, "start": 14.64, "end": 16.2, "text": " it has no clue.", "tokens": [51096, 309, 575, 572, 13602, 13, 51174], "temperature": 0.0, "avg_logprob": -0.17598097441626376, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.004197871778160334}, {"id": 7, "seek": 0, "start": 16.2, "end": 18.400000000000002, "text": " But if you feed it the data that is", "tokens": [51174, 583, 498, 291, 3154, 309, 264, 1412, 300, 307, 51284], "temperature": 0.0, "avg_logprob": -0.17598097441626376, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.004197871778160334}, {"id": 8, "seek": 0, "start": 18.400000000000002, "end": 20.52, "text": " in stored in your vector database,", "tokens": [51284, 294, 12187, 294, 428, 8062, 8149, 11, 51390], "temperature": 0.0, "avg_logprob": -0.17598097441626376, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.004197871778160334}, {"id": 9, "seek": 0, "start": 20.52, "end": 25.16, "text": " and maybe you specify that Erica has a dog and her name is Bowen,", "tokens": [51390, 293, 1310, 291, 16500, 300, 37429, 575, 257, 3000, 293, 720, 1315, 307, 12903, 268, 11, 51622], "temperature": 0.0, "avg_logprob": -0.17598097441626376, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.004197871778160334}, {"id": 10, "seek": 0, "start": 25.16, "end": 27.2, "text": " Chatchapiti is able to retrieve", "tokens": [51622, 761, 852, 569, 8707, 307, 1075, 281, 30254, 51724], "temperature": 0.0, "avg_logprob": -0.17598097441626376, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.004197871778160334}, {"id": 11, "seek": 2720, "start": 27.2, "end": 30.32, "text": " that relevant context and properly answer your question.", "tokens": [50364, 300, 7340, 4319, 293, 6108, 1867, 428, 1168, 13, 50520], "temperature": 0.0, "avg_logprob": -0.14205931000790353, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.004330730997025967}, {"id": 12, "seek": 2720, "start": 30.32, "end": 32.519999999999996, "text": " In this video, I'll go over how you can", "tokens": [50520, 682, 341, 960, 11, 286, 603, 352, 670, 577, 291, 393, 50630], "temperature": 0.0, "avg_logprob": -0.14205931000790353, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.004330730997025967}, {"id": 13, "seek": 2720, "start": 32.519999999999996, "end": 37.28, "text": " chunk your text data to ensure that the language model is one,", "tokens": [50630, 16635, 428, 2487, 1412, 281, 5586, 300, 264, 2856, 2316, 307, 472, 11, 50868], "temperature": 0.0, "avg_logprob": -0.14205931000790353, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.004330730997025967}, {"id": 14, "seek": 2720, "start": 37.28, "end": 40.28, "text": " receiving the most relevant information,", "tokens": [50868, 10040, 264, 881, 7340, 1589, 11, 51018], "temperature": 0.0, "avg_logprob": -0.14205931000790353, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.004330730997025967}, {"id": 15, "seek": 2720, "start": 40.28, "end": 43.239999999999995, "text": " maybe instead of feeding it the whole PDF document,", "tokens": [51018, 1310, 2602, 295, 12919, 309, 264, 1379, 17752, 4166, 11, 51166], "temperature": 0.0, "avg_logprob": -0.14205931000790353, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.004330730997025967}, {"id": 16, "seek": 2720, "start": 43.239999999999995, "end": 44.76, "text": " you feed it by elements,", "tokens": [51166, 291, 3154, 309, 538, 4959, 11, 51242], "temperature": 0.0, "avg_logprob": -0.14205931000790353, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.004330730997025967}, {"id": 17, "seek": 2720, "start": 44.76, "end": 46.239999999999995, "text": " which I have a visual for,", "tokens": [51242, 597, 286, 362, 257, 5056, 337, 11, 51316], "temperature": 0.0, "avg_logprob": -0.14205931000790353, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.004330730997025967}, {"id": 18, "seek": 2720, "start": 46.239999999999995, "end": 49.519999999999996, "text": " and then two, you don't want to go over the context window,", "tokens": [51316, 293, 550, 732, 11, 291, 500, 380, 528, 281, 352, 670, 264, 4319, 4910, 11, 51480], "temperature": 0.0, "avg_logprob": -0.14205931000790353, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.004330730997025967}, {"id": 19, "seek": 2720, "start": 49.519999999999996, "end": 52.519999999999996, "text": " and this is dependent on which model that you are using.", "tokens": [51480, 293, 341, 307, 12334, 322, 597, 2316, 300, 291, 366, 1228, 13, 51630], "temperature": 0.0, "avg_logprob": -0.14205931000790353, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.004330730997025967}, {"id": 20, "seek": 2720, "start": 52.519999999999996, "end": 54.16, "text": " Starting with the text splitter.", "tokens": [51630, 16217, 365, 264, 2487, 4732, 3904, 13, 51712], "temperature": 0.0, "avg_logprob": -0.14205931000790353, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.004330730997025967}, {"id": 21, "seek": 5416, "start": 54.199999999999996, "end": 57.239999999999995, "text": " So what this does is it takes the PDF document,", "tokens": [50366, 407, 437, 341, 775, 307, 309, 2516, 264, 17752, 4166, 11, 50518], "temperature": 0.0, "avg_logprob": -0.16289678912296474, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.00028683789423666894}, {"id": 22, "seek": 5416, "start": 57.239999999999995, "end": 59.72, "text": " which we have here on the left-hand side,", "tokens": [50518, 597, 321, 362, 510, 322, 264, 1411, 12, 5543, 1252, 11, 50642], "temperature": 0.0, "avg_logprob": -0.16289678912296474, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.00028683789423666894}, {"id": 23, "seek": 5416, "start": 59.72, "end": 63.4, "text": " and it takes the chunks depending on the model", "tokens": [50642, 293, 309, 2516, 264, 24004, 5413, 322, 264, 2316, 50826], "temperature": 0.0, "avg_logprob": -0.16289678912296474, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.00028683789423666894}, {"id": 24, "seek": 5416, "start": 63.4, "end": 65.44, "text": " or the chunk size that you defined.", "tokens": [50826, 420, 264, 16635, 2744, 300, 291, 7642, 13, 50928], "temperature": 0.0, "avg_logprob": -0.16289678912296474, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.00028683789423666894}, {"id": 25, "seek": 5416, "start": 65.44, "end": 72.12, "text": " So for example, GPT 3.5 Turbo 16K has a 16,000 token window,", "tokens": [50928, 407, 337, 1365, 11, 26039, 51, 805, 13, 20, 35848, 3165, 42, 575, 257, 3165, 11, 1360, 14862, 4910, 11, 51262], "temperature": 0.0, "avg_logprob": -0.16289678912296474, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.00028683789423666894}, {"id": 26, "seek": 5416, "start": 72.12, "end": 76.84, "text": " but other models have either a smaller or larger window size.", "tokens": [51262, 457, 661, 5245, 362, 2139, 257, 4356, 420, 4833, 4910, 2744, 13, 51498], "temperature": 0.0, "avg_logprob": -0.16289678912296474, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.00028683789423666894}, {"id": 27, "seek": 5416, "start": 76.84, "end": 79.64, "text": " So just what this is doing is just splitting up the text", "tokens": [51498, 407, 445, 437, 341, 307, 884, 307, 445, 30348, 493, 264, 2487, 51638], "temperature": 0.0, "avg_logprob": -0.16289678912296474, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.00028683789423666894}, {"id": 28, "seek": 5416, "start": 79.64, "end": 81.03999999999999, "text": " once it hits that limit,", "tokens": [51638, 1564, 309, 8664, 300, 4948, 11, 51708], "temperature": 0.0, "avg_logprob": -0.16289678912296474, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.00028683789423666894}, {"id": 29, "seek": 8104, "start": 81.04, "end": 84.72000000000001, "text": " and then boom, we have chunk one, which is the blue text,", "tokens": [50364, 293, 550, 9351, 11, 321, 362, 16635, 472, 11, 597, 307, 264, 3344, 2487, 11, 50548], "temperature": 0.0, "avg_logprob": -0.09773521423339844, "compression_ratio": 1.7429906542056075, "no_speech_prob": 0.00035695265978574753}, {"id": 30, "seek": 8104, "start": 84.72000000000001, "end": 88.32000000000001, "text": " and then it moves right on to the green window,", "tokens": [50548, 293, 550, 309, 6067, 558, 322, 281, 264, 3092, 4910, 11, 50728], "temperature": 0.0, "avg_logprob": -0.09773521423339844, "compression_ratio": 1.7429906542056075, "no_speech_prob": 0.00035695265978574753}, {"id": 31, "seek": 8104, "start": 88.32000000000001, "end": 90.0, "text": " which is the next chunk.", "tokens": [50728, 597, 307, 264, 958, 16635, 13, 50812], "temperature": 0.0, "avg_logprob": -0.09773521423339844, "compression_ratio": 1.7429906542056075, "no_speech_prob": 0.00035695265978574753}, {"id": 32, "seek": 8104, "start": 90.0, "end": 93.96000000000001, "text": " The rolling window complements the text splitter extremely well,", "tokens": [50812, 440, 9439, 4910, 715, 17988, 264, 2487, 4732, 3904, 4664, 731, 11, 51010], "temperature": 0.0, "avg_logprob": -0.09773521423339844, "compression_ratio": 1.7429906542056075, "no_speech_prob": 0.00035695265978574753}, {"id": 33, "seek": 8104, "start": 93.96000000000001, "end": 97.04, "text": " and this is because once chunk one is finished,", "tokens": [51010, 293, 341, 307, 570, 1564, 16635, 472, 307, 4335, 11, 51164], "temperature": 0.0, "avg_logprob": -0.09773521423339844, "compression_ratio": 1.7429906542056075, "no_speech_prob": 0.00035695265978574753}, {"id": 34, "seek": 8104, "start": 97.04, "end": 101.4, "text": " chunk two will begin with a few tokens or characters", "tokens": [51164, 16635, 732, 486, 1841, 365, 257, 1326, 22667, 420, 4342, 51382], "temperature": 0.0, "avg_logprob": -0.09773521423339844, "compression_ratio": 1.7429906542056075, "no_speech_prob": 0.00035695265978574753}, {"id": 35, "seek": 8104, "start": 101.4, "end": 103.68, "text": " that is included in chunk one.", "tokens": [51382, 300, 307, 5556, 294, 16635, 472, 13, 51496], "temperature": 0.0, "avg_logprob": -0.09773521423339844, "compression_ratio": 1.7429906542056075, "no_speech_prob": 0.00035695265978574753}, {"id": 36, "seek": 8104, "start": 103.68, "end": 106.80000000000001, "text": " So in this example, you can see the blue text", "tokens": [51496, 407, 294, 341, 1365, 11, 291, 393, 536, 264, 3344, 2487, 51652], "temperature": 0.0, "avg_logprob": -0.09773521423339844, "compression_ratio": 1.7429906542056075, "no_speech_prob": 0.00035695265978574753}, {"id": 37, "seek": 10680, "start": 106.84, "end": 111.56, "text": " from this document is the beginning or the start of chunk two,", "tokens": [50366, 490, 341, 4166, 307, 264, 2863, 420, 264, 722, 295, 16635, 732, 11, 50602], "temperature": 0.0, "avg_logprob": -0.11047534473606797, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.005059876944869757}, {"id": 38, "seek": 10680, "start": 111.56, "end": 114.03999999999999, "text": " and this is very important in documents", "tokens": [50602, 293, 341, 307, 588, 1021, 294, 8512, 50726], "temperature": 0.0, "avg_logprob": -0.11047534473606797, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.005059876944869757}, {"id": 39, "seek": 10680, "start": 114.03999999999999, "end": 117.2, "text": " where the second sentence doesn't make sense", "tokens": [50726, 689, 264, 1150, 8174, 1177, 380, 652, 2020, 50884], "temperature": 0.0, "avg_logprob": -0.11047534473606797, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.005059876944869757}, {"id": 40, "seek": 10680, "start": 117.2, "end": 120.84, "text": " because it doesn't have the information from sentence one.", "tokens": [50884, 570, 309, 1177, 380, 362, 264, 1589, 490, 8174, 472, 13, 51066], "temperature": 0.0, "avg_logprob": -0.11047534473606797, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.005059876944869757}, {"id": 41, "seek": 10680, "start": 120.84, "end": 124.0, "text": " Lama index, Lang chain, and Haystack,", "tokens": [51066, 441, 2404, 8186, 11, 13313, 5021, 11, 293, 8721, 372, 501, 11, 51224], "temperature": 0.0, "avg_logprob": -0.11047534473606797, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.005059876944869757}, {"id": 42, "seek": 10680, "start": 124.0, "end": 127.4, "text": " they all three have this implemented in their frameworks,", "tokens": [51224, 436, 439, 1045, 362, 341, 12270, 294, 641, 29834, 11, 51394], "temperature": 0.0, "avg_logprob": -0.11047534473606797, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.005059876944869757}, {"id": 43, "seek": 10680, "start": 127.4, "end": 128.68, "text": " and at the end of this video,", "tokens": [51394, 293, 412, 264, 917, 295, 341, 960, 11, 51458], "temperature": 0.0, "avg_logprob": -0.11047534473606797, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.005059876944869757}, {"id": 44, "seek": 10680, "start": 128.68, "end": 131.56, "text": " I'll share it with you with the link to the documentation", "tokens": [51458, 286, 603, 2073, 309, 365, 291, 365, 264, 2113, 281, 264, 14333, 51602], "temperature": 0.0, "avg_logprob": -0.11047534473606797, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.005059876944869757}, {"id": 45, "seek": 10680, "start": 131.56, "end": 133.0, "text": " so you can test this out for yourself.", "tokens": [51602, 370, 291, 393, 1500, 341, 484, 337, 1803, 13, 51674], "temperature": 0.0, "avg_logprob": -0.11047534473606797, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.005059876944869757}, {"id": 46, "seek": 10680, "start": 133.0, "end": 135.84, "text": " Moving on to how you would chunk PDF documents.", "tokens": [51674, 14242, 322, 281, 577, 291, 576, 16635, 17752, 8512, 13, 51816], "temperature": 0.0, "avg_logprob": -0.11047534473606797, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.005059876944869757}, {"id": 47, "seek": 13584, "start": 135.88, "end": 138.44, "text": " So previously, Chukrin I partnered up", "tokens": [50366, 407, 8046, 11, 761, 2034, 12629, 286, 29865, 493, 50494], "temperature": 0.0, "avg_logprob": -0.13295458971969482, "compression_ratio": 1.6213235294117647, "no_speech_prob": 0.0004728140193037689}, {"id": 48, "seek": 13584, "start": 138.44, "end": 140.8, "text": " on creating a demo for unstructured.", "tokens": [50494, 322, 4084, 257, 10723, 337, 18799, 46847, 13, 50612], "temperature": 0.0, "avg_logprob": -0.13295458971969482, "compression_ratio": 1.6213235294117647, "no_speech_prob": 0.0004728140193037689}, {"id": 49, "seek": 13584, "start": 140.8, "end": 143.64000000000001, "text": " So what we did was we ingested PDF documents,", "tokens": [50612, 407, 437, 321, 630, 390, 321, 3957, 21885, 17752, 8512, 11, 50754], "temperature": 0.0, "avg_logprob": -0.13295458971969482, "compression_ratio": 1.6213235294117647, "no_speech_prob": 0.0004728140193037689}, {"id": 50, "seek": 13584, "start": 143.64000000000001, "end": 145.8, "text": " which were two research papers,", "tokens": [50754, 597, 645, 732, 2132, 10577, 11, 50862], "temperature": 0.0, "avg_logprob": -0.13295458971969482, "compression_ratio": 1.6213235294117647, "no_speech_prob": 0.0004728140193037689}, {"id": 51, "seek": 13584, "start": 145.8, "end": 148.6, "text": " and then we chunked it by the elements,", "tokens": [50862, 293, 550, 321, 16635, 292, 309, 538, 264, 4959, 11, 51002], "temperature": 0.0, "avg_logprob": -0.13295458971969482, "compression_ratio": 1.6213235294117647, "no_speech_prob": 0.0004728140193037689}, {"id": 52, "seek": 13584, "start": 148.6, "end": 150.16, "text": " which unstructured has.", "tokens": [51002, 597, 18799, 46847, 575, 13, 51080], "temperature": 0.0, "avg_logprob": -0.13295458971969482, "compression_ratio": 1.6213235294117647, "no_speech_prob": 0.0004728140193037689}, {"id": 53, "seek": 13584, "start": 150.16, "end": 152.44, "text": " And if you haven't used unstructured before,", "tokens": [51080, 400, 498, 291, 2378, 380, 1143, 18799, 46847, 949, 11, 51194], "temperature": 0.0, "avg_logprob": -0.13295458971969482, "compression_ratio": 1.6213235294117647, "no_speech_prob": 0.0004728140193037689}, {"id": 54, "seek": 13584, "start": 152.44, "end": 156.32, "text": " I highly recommend checking it out along with Lama index.", "tokens": [51194, 286, 5405, 2748, 8568, 309, 484, 2051, 365, 441, 2404, 8186, 13, 51388], "temperature": 0.0, "avg_logprob": -0.13295458971969482, "compression_ratio": 1.6213235294117647, "no_speech_prob": 0.0004728140193037689}, {"id": 55, "seek": 13584, "start": 156.32, "end": 160.16, "text": " So what it is doing is it takes the PDF heading.", "tokens": [51388, 407, 437, 309, 307, 884, 307, 309, 2516, 264, 17752, 9864, 13, 51580], "temperature": 0.0, "avg_logprob": -0.13295458971969482, "compression_ratio": 1.6213235294117647, "no_speech_prob": 0.0004728140193037689}, {"id": 56, "seek": 13584, "start": 160.16, "end": 162.24, "text": " So in this example, I have the abstract,", "tokens": [51580, 407, 294, 341, 1365, 11, 286, 362, 264, 12649, 11, 51684], "temperature": 0.0, "avg_logprob": -0.13295458971969482, "compression_ratio": 1.6213235294117647, "no_speech_prob": 0.0004728140193037689}, {"id": 57, "seek": 13584, "start": 162.24, "end": 163.8, "text": " introduction, and related work.", "tokens": [51684, 9339, 11, 293, 4077, 589, 13, 51762], "temperature": 0.0, "avg_logprob": -0.13295458971969482, "compression_ratio": 1.6213235294117647, "no_speech_prob": 0.0004728140193037689}, {"id": 58, "seek": 16380, "start": 163.8, "end": 166.68, "text": " So each section will have its own chunk.", "tokens": [50364, 407, 1184, 3541, 486, 362, 1080, 1065, 16635, 13, 50508], "temperature": 0.0, "avg_logprob": -0.11711490252786431, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.00035694631515070796}, {"id": 59, "seek": 16380, "start": 166.68, "end": 169.72, "text": " And this is very important when making queries", "tokens": [50508, 400, 341, 307, 588, 1021, 562, 1455, 24109, 50660], "temperature": 0.0, "avg_logprob": -0.11711490252786431, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.00035694631515070796}, {"id": 60, "seek": 16380, "start": 169.72, "end": 171.84, "text": " that are specific to title.", "tokens": [50660, 300, 366, 2685, 281, 4876, 13, 50766], "temperature": 0.0, "avg_logprob": -0.11711490252786431, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.00035694631515070796}, {"id": 61, "seek": 16380, "start": 171.84, "end": 175.28, "text": " So this is that kind of semantic region of searching.", "tokens": [50766, 407, 341, 307, 300, 733, 295, 47982, 4458, 295, 10808, 13, 50938], "temperature": 0.0, "avg_logprob": -0.11711490252786431, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.00035694631515070796}, {"id": 62, "seek": 16380, "start": 175.28, "end": 177.52, "text": " And maybe you'd have like the abstract property,", "tokens": [50938, 400, 1310, 291, 1116, 362, 411, 264, 12649, 4707, 11, 51050], "temperature": 0.0, "avg_logprob": -0.11711490252786431, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.00035694631515070796}, {"id": 63, "seek": 16380, "start": 177.52, "end": 179.52, "text": " introduction, property, et cetera.", "tokens": [51050, 9339, 11, 4707, 11, 1030, 11458, 13, 51150], "temperature": 0.0, "avg_logprob": -0.11711490252786431, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.00035694631515070796}, {"id": 64, "seek": 16380, "start": 179.52, "end": 181.08, "text": " And this is one way to make sure", "tokens": [51150, 400, 341, 307, 472, 636, 281, 652, 988, 51228], "temperature": 0.0, "avg_logprob": -0.11711490252786431, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.00035694631515070796}, {"id": 65, "seek": 16380, "start": 181.08, "end": 184.0, "text": " that you have extremely relevant information.", "tokens": [51228, 300, 291, 362, 4664, 7340, 1589, 13, 51374], "temperature": 0.0, "avg_logprob": -0.11711490252786431, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.00035694631515070796}, {"id": 66, "seek": 16380, "start": 184.0, "end": 186.4, "text": " If you're interested in using unstructured", "tokens": [51374, 759, 291, 434, 3102, 294, 1228, 18799, 46847, 51494], "temperature": 0.0, "avg_logprob": -0.11711490252786431, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.00035694631515070796}, {"id": 67, "seek": 16380, "start": 186.4, "end": 189.96, "text": " with Lama index, so if you start typing in unstructured,", "tokens": [51494, 365, 441, 2404, 8186, 11, 370, 498, 291, 722, 18444, 294, 18799, 46847, 11, 51672], "temperature": 0.0, "avg_logprob": -0.11711490252786431, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.00035694631515070796}, {"id": 68, "seek": 16380, "start": 189.96, "end": 193.08, "text": " you can find the file loader here.", "tokens": [51672, 291, 393, 915, 264, 3991, 3677, 260, 510, 13, 51828], "temperature": 0.0, "avg_logprob": -0.11711490252786431, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.00035694631515070796}, {"id": 69, "seek": 19308, "start": 193.08, "end": 196.0, "text": " And this is where you can ingest TXT files,", "tokens": [50364, 400, 341, 307, 689, 291, 393, 3957, 377, 314, 20542, 7098, 11, 50510], "temperature": 0.0, "avg_logprob": -0.11367660410263959, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.00014425050176214427}, {"id": 70, "seek": 19308, "start": 196.0, "end": 198.28, "text": " doc, PowerPoint, JPEG, all of it.", "tokens": [50510, 3211, 11, 25584, 11, 508, 5208, 38, 11, 439, 295, 309, 13, 50624], "temperature": 0.0, "avg_logprob": -0.11367660410263959, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.00014425050176214427}, {"id": 71, "seek": 19308, "start": 198.28, "end": 201.48000000000002, "text": " Then moving on to the Lama index node parser.", "tokens": [50624, 1396, 2684, 322, 281, 264, 441, 2404, 8186, 9984, 21156, 260, 13, 50784], "temperature": 0.0, "avg_logprob": -0.11367660410263959, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.00014425050176214427}, {"id": 72, "seek": 19308, "start": 201.48000000000002, "end": 204.0, "text": " So this is where you would define that chunk size", "tokens": [50784, 407, 341, 307, 689, 291, 576, 6964, 300, 16635, 2744, 50910], "temperature": 0.0, "avg_logprob": -0.11367660410263959, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.00014425050176214427}, {"id": 73, "seek": 19308, "start": 204.0, "end": 207.16000000000003, "text": " that I talked about, and then also the chunk overlap.", "tokens": [50910, 300, 286, 2825, 466, 11, 293, 550, 611, 264, 16635, 19959, 13, 51068], "temperature": 0.0, "avg_logprob": -0.11367660410263959, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.00014425050176214427}, {"id": 74, "seek": 19308, "start": 207.16000000000003, "end": 210.24, "text": " So with this chunk overlap of 20 tokens,", "tokens": [51068, 407, 365, 341, 16635, 19959, 295, 945, 22667, 11, 51222], "temperature": 0.0, "avg_logprob": -0.11367660410263959, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.00014425050176214427}, {"id": 75, "seek": 19308, "start": 210.24, "end": 213.12, "text": " it is taking the 20 tokens from the previous chunk", "tokens": [51222, 309, 307, 1940, 264, 945, 22667, 490, 264, 3894, 16635, 51366], "temperature": 0.0, "avg_logprob": -0.11367660410263959, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.00014425050176214427}, {"id": 76, "seek": 19308, "start": 213.12, "end": 215.08, "text": " into the following chunk.", "tokens": [51366, 666, 264, 3480, 16635, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11367660410263959, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.00014425050176214427}, {"id": 77, "seek": 19308, "start": 215.08, "end": 217.16000000000003, "text": " And this is a lane chain documentation", "tokens": [51464, 400, 341, 307, 257, 12705, 5021, 14333, 51568], "temperature": 0.0, "avg_logprob": -0.11367660410263959, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.00014425050176214427}, {"id": 78, "seek": 19308, "start": 217.16000000000003, "end": 219.64000000000001, "text": " on where they have their text splitters.", "tokens": [51568, 322, 689, 436, 362, 641, 2487, 7472, 1559, 13, 51692], "temperature": 0.0, "avg_logprob": -0.11367660410263959, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.00014425050176214427}, {"id": 79, "seek": 19308, "start": 219.64000000000001, "end": 222.96, "text": " So they have, I believe, code on how to do this.", "tokens": [51692, 407, 436, 362, 11, 286, 1697, 11, 3089, 322, 577, 281, 360, 341, 13, 51858], "temperature": 0.0, "avg_logprob": -0.11367660410263959, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.00014425050176214427}, {"id": 80, "seek": 22296, "start": 222.96, "end": 226.64000000000001, "text": " And maybe if you want to split the text by the new lines", "tokens": [50364, 400, 1310, 498, 291, 528, 281, 7472, 264, 2487, 538, 264, 777, 3876, 50548], "temperature": 0.0, "avg_logprob": -0.12718879629712587, "compression_ratio": 1.7477477477477477, "no_speech_prob": 7.721596193732694e-05}, {"id": 81, "seek": 22296, "start": 226.64000000000001, "end": 230.8, "text": " or also the chunk overlap and the chunk size as well.", "tokens": [50548, 420, 611, 264, 16635, 19959, 293, 264, 16635, 2744, 382, 731, 13, 50756], "temperature": 0.0, "avg_logprob": -0.12718879629712587, "compression_ratio": 1.7477477477477477, "no_speech_prob": 7.721596193732694e-05}, {"id": 82, "seek": 22296, "start": 230.8, "end": 233.08, "text": " And then ending with haystack.", "tokens": [50756, 400, 550, 8121, 365, 4842, 372, 501, 13, 50870], "temperature": 0.0, "avg_logprob": -0.12718879629712587, "compression_ratio": 1.7477477477477477, "no_speech_prob": 7.721596193732694e-05}, {"id": 83, "seek": 22296, "start": 233.08, "end": 235.20000000000002, "text": " They have the chunk size,", "tokens": [50870, 814, 362, 264, 16635, 2744, 11, 50976], "temperature": 0.0, "avg_logprob": -0.12718879629712587, "compression_ratio": 1.7477477477477477, "no_speech_prob": 7.721596193732694e-05}, {"id": 84, "seek": 22296, "start": 235.20000000000002, "end": 238.24, "text": " and then again the split overlap.", "tokens": [50976, 293, 550, 797, 264, 7472, 19959, 13, 51128], "temperature": 0.0, "avg_logprob": -0.12718879629712587, "compression_ratio": 1.7477477477477477, "no_speech_prob": 7.721596193732694e-05}, {"id": 85, "seek": 22296, "start": 238.24, "end": 239.76000000000002, "text": " So you can check that out.", "tokens": [51128, 407, 291, 393, 1520, 300, 484, 13, 51204], "temperature": 0.0, "avg_logprob": -0.12718879629712587, "compression_ratio": 1.7477477477477477, "no_speech_prob": 7.721596193732694e-05}, {"id": 86, "seek": 22296, "start": 239.76000000000002, "end": 243.36, "text": " And then I want to end this video", "tokens": [51204, 400, 550, 286, 528, 281, 917, 341, 960, 51384], "temperature": 0.0, "avg_logprob": -0.12718879629712587, "compression_ratio": 1.7477477477477477, "no_speech_prob": 7.721596193732694e-05}, {"id": 87, "seek": 22296, "start": 243.36, "end": 246.96, "text": " with recommending you join the Arise workshop", "tokens": [51384, 365, 30559, 291, 3917, 264, 1587, 908, 13541, 51564], "temperature": 0.0, "avg_logprob": -0.12718879629712587, "compression_ratio": 1.7477477477477477, "no_speech_prob": 7.721596193732694e-05}, {"id": 88, "seek": 22296, "start": 246.96, "end": 249.92000000000002, "text": " that I will be hosting along with Aman from Arise", "tokens": [51564, 300, 286, 486, 312, 16058, 2051, 365, 35466, 490, 1587, 908, 51712], "temperature": 0.0, "avg_logprob": -0.12718879629712587, "compression_ratio": 1.7477477477477477, "no_speech_prob": 7.721596193732694e-05}, {"id": 89, "seek": 22296, "start": 249.92000000000002, "end": 251.60000000000002, "text": " and Ronnie from Unstructured.", "tokens": [51712, 293, 46131, 490, 1156, 372, 46847, 13, 51796], "temperature": 0.0, "avg_logprob": -0.12718879629712587, "compression_ratio": 1.7477477477477477, "no_speech_prob": 7.721596193732694e-05}, {"id": 90, "seek": 25160, "start": 251.64, "end": 254.4, "text": " We will be exploring the chunking techniques", "tokens": [50366, 492, 486, 312, 12736, 264, 16635, 278, 7512, 50504], "temperature": 0.0, "avg_logprob": -0.14552027492199915, "compression_ratio": 1.3611111111111112, "no_speech_prob": 0.001955983228981495}, {"id": 91, "seek": 25160, "start": 254.4, "end": 258.12, "text": " and re-ranking for enhancing your retrieval", "tokens": [50504, 293, 319, 12, 20479, 278, 337, 36579, 428, 19817, 3337, 50690], "temperature": 0.0, "avg_logprob": -0.14552027492199915, "compression_ratio": 1.3611111111111112, "no_speech_prob": 0.001955983228981495}, {"id": 92, "seek": 25160, "start": 258.12, "end": 261.04, "text": " and the results in your FAG application.", "tokens": [50690, 293, 264, 3542, 294, 428, 19894, 38, 3861, 13, 50836], "temperature": 0.0, "avg_logprob": -0.14552027492199915, "compression_ratio": 1.3611111111111112, "no_speech_prob": 0.001955983228981495}, {"id": 93, "seek": 25160, "start": 261.04, "end": 262.84, "text": " I hope you guys are able to join.", "tokens": [50836, 286, 1454, 291, 1074, 366, 1075, 281, 3917, 13, 50926], "temperature": 0.0, "avg_logprob": -0.14552027492199915, "compression_ratio": 1.3611111111111112, "no_speech_prob": 0.001955983228981495}, {"id": 94, "seek": 25160, "start": 262.84, "end": 264.28, "text": " And I'll see you next time.", "tokens": [50926, 400, 286, 603, 536, 291, 958, 565, 13, 50998], "temperature": 0.0, "avg_logprob": -0.14552027492199915, "compression_ratio": 1.3611111111111112, "no_speech_prob": 0.001955983228981495}, {"id": 95, "seek": 25160, "start": 264.28, "end": 265.12, "text": " Bye.", "tokens": [50998, 4621, 13, 51040], "temperature": 0.0, "avg_logprob": -0.14552027492199915, "compression_ratio": 1.3611111111111112, "no_speech_prob": 0.001955983228981495}], "language": "en"}